{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Research Synthesizer Demo\n\nThis notebook demonstrates an **Agentic RAG system** that can:\n1. **Ingest** research papers from arxiv\n2. **Index** them in a vector database (Chroma)\n3. **Query** using basic RAG or advanced agentic approach\n4. **Evaluate** both approaches with LLM-as-judge\n\n## What makes this \"Agentic\"?\n- **Query Decomposition**: Complex questions are broken into simpler sub-questions\n- **Multi-hop Retrieval**: Each sub-question retrieves relevant context\n- **Synthesis**: Sub-answers are combined into a comprehensive response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import our modules and load the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\nfrom src.retriever import load_index, retrieve\nfrom src.query_engine import create_query_engine, query_with_sources\nfrom src.agent import create_synthesis_agent\nfrom src.decomposition import decompose_query\nfrom src.config import get_llm_with_fallback\n\n# Change to project root directory (parent of notebooks/)\nproject_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\nos.chdir(project_root)\nsys.path.insert(0, str(project_root))\n\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1990.52it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the vector index\n",
    "print(\"Loading index...\")\n",
    "index = load_index()\n",
    "print(\"Index loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Retrieval\n",
    "\n",
    "Let's start with simple retrieval to see what documents match a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Retrieved Chunks:\n",
      "============================================================\n",
      "\n",
      "--- Result 1 (score: 0.4190) ---\n",
      "Title: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "Text: A\n",
      "promising approach to mitigating these challenges is retrieval-augmented generation (RAG), which\n",
      "enhances the generation process by incorporating real-world images as additional references [8, 3].\n",
      "While RAG has been extensively explored in the language domain [23, 13], its application to image\n",
      "and...\n",
      "\n",
      "--- Result 2 (score: 0.3763) ---\n",
      "Title: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "Text: By comprehensively examining images produced by Im-\n",
      "ageRAG alongside their corresponding retrieved reference images, we identify two critical challenges\n",
      "inherent in image-level retrieval augmentation approaches. First, these methods tend to overcopy\n",
      "irrelevant visual elements from retrieved referenc...\n",
      "\n",
      "--- Result 3 (score: 0.3564) ---\n",
      "Title: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "Text: Retrieval-augmented multimodal\n",
      "language modeling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\n",
      "Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning,\n",
      "ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Basic retrieval\n",
    "results = retrieve(\"What is retrieval augmented generation?\", index, top_k=3)\n",
    "\n",
    "print(\"Top 3 Retrieved Chunks:\")\n",
    "print(\"=\" * 60)\n",
    "for i, node in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} (score: {node.score:.4f}) ---\")\n",
    "    print(f\"Title: {node.node.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Text: {node.node.get_content()[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic RAG (Baseline)\n",
    "\n",
    "Now let's use the full RAG pipeline with answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1960.71it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is retrieval augmented generation?\n",
      "\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      " Retrieval-augmented generation (RAG) is a paradigm that enhances generative models by incorporating external knowledge or references during the generation process. Originally developed for natural language processing to retrieve relevant documents, it extends to image generation by utilizing external visual references—ranging from full images to finer granularities such as patches—to condition and guide the creation of visual content. In autoregressive generation frameworks, patch-level retrieval methods offer distinct advantages over image-level approaches, providing more precise conditioning that enhances generation quality and control.\n",
      "\n",
      "\n",
      "Sources (5)\n",
      "  - AR-RAG: Autoregressive Retrieval Augmentation for Image Generation (score: 0.4190)\n",
      "  - AR-RAG: Autoregressive Retrieval Augmentation for Image Generation (score: 0.3763)\n",
      "  - AR-RAG: Autoregressive Retrieval Augmentation for Image Generation (score: 0.3564)\n",
      "  - AR-RAG: Autoregressive Retrieval Augmentation for Image Generation (score: 0.3520)\n",
      "  - Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation (score: 0.3488)\n"
     ]
    }
   ],
   "source": [
    "# Create basic query engine\n",
    "query_engine = create_query_engine(index)\n",
    "\n",
    "# Ask a simple question\n",
    "question = \"What is retrieval augmented generation?\"\n",
    "result = query_with_sources(query_engine, question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\n\\nSources ({len(result['sources'])})\")\n",
    "for s in result['sources']:\n",
    "    print(f\"  - {s['title']} (score: {s['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Decomposition\n",
    "\n",
    "For complex questions, we first break them into sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n",
      "Original Question:\n",
      "Compare different retrieval methods and their effectiveness for RAG systems\n",
      "\n",
      "============================================================\n",
      "\n",
      "Decomposed into sub-questions:\n",
      "  1. What are the primary categories of retrieval methods used in RAG systems (e.g., sparse, dense, and hybrid approaches)?\n",
      "  2. What evaluation metrics are commonly used to measure retrieval effectiveness in RAG systems?\n",
      "  3. How do different retrieval methods compare in terms of retrieval accuracy and result relevance for RAG applications?\n",
      "  4. What are the computational efficiency and scalability trade-offs between different retrieval methods in RAG systems?\n"
     ]
    }
   ],
   "source": [
    "# Get LLM for decomposition\n",
    "llm = get_llm_with_fallback()\n",
    "\n",
    "# Complex question\n",
    "complex_question = \"Compare different retrieval methods and their effectiveness for RAG systems\"\n",
    "\n",
    "# Decompose\n",
    "sub_questions = decompose_query(complex_question, llm)\n",
    "\n",
    "print(f\"Original Question:\\n{complex_question}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nDecomposed into sub-questions:\")\n",
    "for i, q in enumerate(sub_questions):\n",
    "    print(f\"  {i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agentic RAG (Synthesis Agent)\n",
    "\n",
    "The synthesis agent:\n",
    "1. Decomposes the query\n",
    "2. Answers each sub-question with retrieval\n",
    "3. Synthesizes all answers into a final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1615.30it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2076.24it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do papers say about chunking strategies and their impact on RAG performance?\n",
      "\n",
      "============================================================\n",
      "Processing...\n",
      "\n",
      "Decomposing query: What do papers say about chunking strategies and their impact on RAG performance?\n",
      "Sub-questions: ['What specific chunking strategies (e.g., fixed-size, semantic, recursive, agentic) have been proposed and evaluated in RAG research literature?', 'How do chunk size and overlap parameters affect retrieval accuracy and recall in RAG systems?', 'What impact do different chunking approaches have on the factual accuracy and coherence of generated responses in RAG pipelines?', 'What comparative analyses exist regarding the trade-offs between semantic chunking versus fixed-size chunking for different document types in RAG?']\n",
      "\n",
      "Answering sub-question 1: What specific chunking strategies (e.g., fixed-size, semantic, recursive, agentic) have been proposed and evaluated in RAG research literature?\n",
      "\n",
      "Answering sub-question 2: How do chunk size and overlap parameters affect retrieval accuracy and recall in RAG systems?\n",
      "\n",
      "Answering sub-question 3: What impact do different chunking approaches have on the factual accuracy and coherence of generated responses in RAG pipelines?\n",
      "\n",
      "Answering sub-question 4: What comparative analyses exist regarding the trade-offs between semantic chunking versus fixed-size chunking for different document types in RAG?\n",
      "\n",
      "Synthesizing final answer...\n"
     ]
    }
   ],
   "source": [
    "# Create synthesis agent\n",
    "agent = create_synthesis_agent(index)\n",
    "\n",
    "# Ask a complex question\n",
    "complex_question = \"What do papers say about chunking strategies and their impact on RAG performance?\"\n",
    "\n",
    "print(f\"Question: {complex_question}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Processing...\\n\")\n",
    "\n",
    "result = agent(complex_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-questions generated:\n",
      "  1. What specific chunking strategies (e.g., fixed-size, semantic, recursive, agentic) have been proposed and evaluated in RAG research literature?\n",
      "  2. How do chunk size and overlap parameters affect retrieval accuracy and recall in RAG systems?\n",
      "  3. What impact do different chunking approaches have on the factual accuracy and coherence of generated responses in RAG pipelines?\n",
      "  4. What comparative analyses exist regarding the trade-offs between semantic chunking versus fixed-size chunking for different document types in RAG?\n",
      "\n",
      "============================================================\n",
      "\n",
      "Final Synthesized Answer:\n",
      " The provided research context offers limited direct discussion of specific text chunking strategies (e.g., fixed-size, semantic, recursive, or agentic chunking) as explicitly defined in the RAG literature. However, the available documents provide relevant insights regarding **retrieval granularity** and **structured document processing** that inform the impact of chunking approaches on RAG performance.\n",
      "\n",
      "## Retrieval Granularity: Fine-Grained vs. Coarse-Grained Approaches\n",
      "\n",
      "The literature emphasizes that the granularity of retrieval units significantly impacts both retrieval accuracy and generation quality:\n",
      "\n",
      "**Fine-Grained Units (Patch-Level/Section-Level):**\n",
      "- **Improved Relevance and Precision**: Fine-grained retrieval demonstrates substantial advantages over coarse-grained approaches by enabling selective incorporation of specific elements while maintaining independence from irrelevant contextual features. This prevents \"over-copying\" of unintended details or stylistic patterns that contradict input requirements (A2).\n",
      "- **Dynamic Contextual Adaptation**: Fine-grained approaches allow retrieval mechanisms to respond to local, evolving context during generation. By conditioning retrieval on already-generated surrounding content (such as using spatial neighbors in patch-level retrieval or section context in documents), systems ensure retrieved references remain dynamically aligned with the generation process, encouraging local semantic coherence (A2, A3).\n",
      "- **Enhanced Factual Accuracy**: By avoiding overcommitment to entire reference documents, fine-grained retrieval reduces the introduction of weakly aligned or irrelevant content that can persist throughout generation, thereby minimizing hallucination of unrelated elements (A3).\n",
      "\n",
      "**Coarse-Grained Units (Full Document/Image-Level):**\n",
      "- **Performance Limitations**: Coarse-grained retrieval relying on larger, static units can introduce irrelevant content that reduces factual accuracy and coherence. Such approaches may result in over-copying of details, stylistic bias, and reduced instruction-following capabilities for complex compositional requirements (A3).\n",
      "\n",
      "## Structured Semantic Sectioning\n",
      "\n",
      "Rather than employing traditional chunking strategies, one approach described involves extracting specific semantic sections—such as abstracts, introductions, and conclusions—rather than processing documents as monolithic units. This structured approach enhances coherence by ensuring retrieved content aligns with specific informational needs of the generation task (A3). This suggests that **semantic structuring** based on document architecture may serve as an effective alternative to arbitrary segmentation for certain RAG applications, such as literature review generation.\n",
      "\n",
      "## Critical Gaps in the Literature\n",
      "\n",
      "The provided context explicitly lacks information regarding:\n",
      "- Comparative analyses between semantic chunking and fixed-size chunking strategies for different document types (A4)\n",
      "- Evaluations of recursive or agentic chunking methodologies (A1)\n",
      "- Specific optimal chunk size and overlap parameters for text-based RAG systems (A2 discusses granularity in the context of image generation patches, not text chunking parameters)\n",
      "\n",
      "## Synthesis\n",
      "\n",
      "While specific chunking algorithms remain unevaluated in the provided context, the evidence suggests that **granularity and contextual alignment** are critical determinants of RAG performance. Fine-grained, dynamically adaptive retrieval units that align with local generation context improve factual accuracy, prevent content bleeding, and enhance coherence compared to coarse-grained, static approaches. For document processing, structured extraction of semantically meaningful sections appears beneficial, though direct comparisons between semantic and fixed-size chunking strategies—and their trade-offs across different document types—are not addressed in the available literature.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sources (2)\n",
      "  - Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation\n",
      "  - AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Sub-questions generated:\")\n",
    "for i, q in enumerate(result['sub_questions']):\n",
    "    print(f\"  {i+1}. {q}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nFinal Synthesized Answer:\")\n",
    "print(result['answer'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nSources ({len(result['sources'])})\")\n",
    "for s in result['sources']:\n",
    "    print(f\"  - {s['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Basic RAG vs Agentic RAG\n",
    "\n",
    "Let's compare both approaches on the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What evaluation metrics are used to assess RAG system performance?\n",
      "\n",
      "============================================================\n",
      "\n",
      "[BASIC RAG]\n",
      "\n",
      "Answer:\n",
      " Evaluation metrics for RAG systems vary by modality and task. For text generation and summarization, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics are standard, including ROUGE-1 and ROUGE-2 for unigram and bigram overlap precision, recall, and F1 scores, as well as ROUGE-L and ROUGE-Lsum for measuring longest common subsequences at the sentence and summary levels. For image generation, GenEval benchmarks assess specific compositional capabilities such as Two Object generati...\n",
      "\n",
      "Sources: 5\n",
      "\n",
      "============================================================\n",
      "\n",
      "[AGENTIC RAG]\n",
      "Decomposing query: What evaluation metrics are used to assess RAG system performance?\n",
      "Sub-questions: ['What metrics are used to evaluate the retrieval component of RAG systems, such as precision@k, recall, and mean reciprocal rank?', 'What metrics assess the quality, fluency, and relevance of the generated text outputs in RAG systems?', 'What specific metrics measure faithfulness, factual consistency, and hallucination rates in RAG-generated responses?', 'What end-to-end evaluation frameworks and benchmarks exist for holistically assessing RAG system performance across different tasks?']\n",
      "\n",
      "Answering sub-question 1: What metrics are used to evaluate the retrieval component of RAG systems, such as precision@k, recall, and mean reciprocal rank?\n",
      "\n",
      "Answering sub-question 2: What metrics assess the quality, fluency, and relevance of the generated text outputs in RAG systems?\n",
      "\n",
      "Answering sub-question 3: What specific metrics measure faithfulness, factual consistency, and hallucination rates in RAG-generated responses?\n",
      "\n",
      "Answering sub-question 4: What end-to-end evaluation frameworks and benchmarks exist for holistically assessing RAG system performance across different tasks?\n",
      "\n",
      "Synthesizing final answer...\n",
      "\n",
      "Sub-questions: 4\n",
      "\n",
      "Answer:\n",
      " Based on the provided materials, evaluation metrics for RAG systems are **domain-specific**, with distinct methodological frameworks for text generation versus image generation tasks, alongside notable gaps in retrieval-specific and faithfulness evaluation.\n",
      "\n",
      "## Text Generation Evaluation\n",
      "For text generation tasks such as automated literature review synthesis, assessment relies primarily on **ROUGE-1 and ROUGE-2 scores** (A2, A4). These metrics measure unigram and bigram overlap between generate...\n",
      "\n",
      "Sources: 3\n"
     ]
    }
   ],
   "source": [
    "# Same question for both\n",
    "test_question = \"What evaluation metrics are used to assess RAG system performance?\"\n",
    "\n",
    "print(\"Question:\", test_question)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Basic RAG\n",
    "print(\"\\n[BASIC RAG]\")\n",
    "basic_result = query_with_sources(query_engine, test_question)\n",
    "print(f\"\\nAnswer:\\n{basic_result['answer'][:500]}...\")\n",
    "print(f\"\\nSources: {len(basic_result['sources'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Agentic RAG\n",
    "print(\"\\n[AGENTIC RAG]\")\n",
    "agentic_result = agent(test_question)\n",
    "print(f\"\\nSub-questions: {len(agentic_result['sub_questions'])}\")\n",
    "print(f\"\\nAnswer:\\n{agentic_result['answer'][:500]}...\")\n",
    "print(f\"\\nSources: {len(agentic_result['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Run a full evaluation comparing both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 test questions\n",
      "  - [low] What is retrieval augmented generation?...\n",
      "  - [low] What embedding models are commonly used for RAG sy...\n",
      "  - [low] How do retrieval methods in RAG systems work?...\n",
      "  - [medium] What are the main challenges in implementing RAG s...\n",
      "  - [medium] Compare dense retrieval versus sparse retrieval me...\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate import load_test_questions, compare_approaches, print_comparison_table\n",
    "\n",
    "# Load test questions (path is relative to project root now)\n",
    "questions = load_test_questions(\"data/test_questions.json\")\n",
    "print(f\"Loaded {len(questions)} test questions\")\n",
    "\n",
    "# Show questions\n",
    "for q in questions[:5]:\n",
    "    print(f\"  - [{q['complexity']}] {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n",
      "\n",
      "============================================================\n",
      "Running Baseline RAG Evaluation\n",
      "============================================================\n",
      "Loading index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1932.61it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n",
      "Creating basic query engine...\n",
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2423.65it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] What is retrieval augmented generation?...\n",
      "\n",
      "[2/2] What embedding models are commonly used for RAG sy...\n",
      "\n",
      "============================================================\n",
      "Running Agentic RAG Evaluation\n",
      "============================================================\n",
      "Loading index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1888.59it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n",
      "Creating synthesis agent...\n",
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1928.40it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: moonshotai/kimi-k2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1290.67it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] What is retrieval augmented generation?...\n",
      "Decomposing query: What is retrieval augmented generation?\n",
      "Sub-questions: ['What is the fundamental definition and core concept of retrieval augmented generation?', 'What are the key components or architectural elements of a retrieval augmented generation system?', 'How does the retrieval augmented generation process work step-by-step?', 'What are the primary advantages and typical use cases of retrieval augmented generation compared to standard language generation?']\n",
      "\n",
      "Answering sub-question 1: What is the fundamental definition and core concept of retrieval augmented generation?\n",
      "\n",
      "Answering sub-question 2: What are the key components or architectural elements of a retrieval augmented generation system?\n",
      "\n",
      "Answering sub-question 3: How does the retrieval augmented generation process work step-by-step?\n",
      "\n",
      "Answering sub-question 4: What are the primary advantages and typical use cases of retrieval augmented generation compared to standard language generation?\n",
      "\n",
      "Synthesizing final answer...\n",
      "\n",
      "[2/2] What embedding models are commonly used for RAG sy...\n",
      "Decomposing query: What embedding models are commonly used for RAG systems?\n",
      "Sub-questions: ['Which open-source embedding models (e.g., BGE, E5, GTE) are most frequently adopted for retrieval in RAG systems?', 'Which commercial embedding APIs are commonly integrated into RAG pipelines?', 'What evaluation benchmarks or metrics are used to compare embedding model effectiveness in RAG contexts?', 'How do domain-specific or fine-tuned embedding models compare to general-purpose models for specialized RAG applications?']\n",
      "\n",
      "Answering sub-question 1: Which open-source embedding models (e.g., BGE, E5, GTE) are most frequently adopted for retrieval in RAG systems?\n",
      "\n",
      "Answering sub-question 2: Which commercial embedding APIs are commonly integrated into RAG pipelines?\n",
      "\n",
      "Answering sub-question 3: What evaluation benchmarks or metrics are used to compare embedding model effectiveness in RAG contexts?\n",
      "\n",
      "Answering sub-question 4: How do domain-specific or fine-tuned embedding models compare to general-purpose models for specialized RAG applications?\n",
      "\n",
      "Synthesizing final answer...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Metric               Baseline     Agentic      Diff        \n",
      "--------------------------------------------------------\n",
      "Relevance            3.50         3.50         +0.00\n",
      "Coverage             3.50         3.50         +0.00\n",
      "Coherence            4.50         4.50         +0.00\n",
      "Avg Time (s)         46.33        262.25       +215.92\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Baseline Overall Score: 3.83/5\n",
      "Agentic Overall Score:  3.83/5\n",
      "Improvement: +0.00 (+0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Run quick evaluation on 2 questions (for demo speed)\n",
    "quick_questions = questions[:2]\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "comparison = compare_approaches(quick_questions)\n",
    "\n",
    "# Print results\n",
    "print_comparison_table(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **Basic Retrieval**: Find relevant document chunks using vector similarity\n",
    "2. **Basic RAG**: Retrieve + Generate answers with citations\n",
    "3. **Query Decomposition**: Break complex questions into simpler parts\n",
    "4. **Agentic RAG**: Multi-hop retrieval with synthesis\n",
    "5. **Evaluation**: Compare approaches with LLM-as-judge\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Basic RAG** is faster but may miss nuanced answers\n",
    "- **Agentic RAG** provides more comprehensive answers for complex questions\n",
    "- The tradeoff is latency vs quality\n",
    "- Query decomposition helps cover multiple aspects of a question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}